{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data, vocab\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from utils import execute_and_time, get_device, itos\n",
    "from preprocess import Batch, embedding_param\n",
    "from model import Transformer\n",
    "from optimize import get_default_optimizer, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE = False\n",
    "logger = logging.getLogger()\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "if LOG_FILE:\n",
    "    file_handler = logging.FileHandler('log.out')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler) \n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-20 18:17:11,301 INFO GPU available, using NO.3.\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/'\n",
    "SAMPLE_DATA_PATH = f'{DATA_PATH}sample_data/'\n",
    "PROCESSED_DATA_PATH = f'{DATA_PATH}processed_data/'\n",
    "\n",
    "pre_trained_vector_type = 'glove.6B.200d' \n",
    "batch_size = 64\n",
    "device = get_device(3)\n",
    "stack_number = 6\n",
    "heads_number = 8\n",
    "fix_length = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.1 s, sys: 192 ms, total: 33.3 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = data.get_tokenizer('spacy')\n",
    "TEXT = data.Field(tokenize=tokenizer, lower=True, eos_token='_eos_', fix_length=fix_length)\n",
    "trn_data_fields = [(\"source\", TEXT), (\"target\", TEXT)]\n",
    "trn, vld = data.TabularDataset.splits(path=f'{SAMPLE_DATA_PATH}',\n",
    "                           train='train_ds.csv', \n",
    "                           validation='valid_ds.csv',\n",
    "                           format='csv', \n",
    "                           skip_header=True, \n",
    "                           fields=trn_data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-20 18:18:12,043 INFO Loading vectors from .vector_cache/glove.6B.200d.txt.pt\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(trn, vectors=pre_trained_vector_type)\n",
    "vocabulary = TEXT.vocab\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits((trn, vld), \n",
    "                                                  batch_sizes=(batch_size, int(batch_size * 1.6)),\n",
    "                                                  device=device, \n",
    "                                                  sort_key=lambda x: len(x.source),\n",
    "                                                  shuffle=True, \n",
    "                                                  sort_within_batch=False, \n",
    "                                                  repeat=True)\n",
    "train_iter = Batch(train_iter, \"source\", \"target\", device, vocabulary)\n",
    "val_iter = Batch(val_iter, \"source\", \"target\", device, vocabulary)\n",
    "# train_iter, val_iter = iter(train_iter_tuple), iter(val_iter_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 4\n",
      "torch.Size([64, 80]) torch.Size([64, 80]) torch.Size([64, 1, 1, 80]) torch.Size([64, 1, 80, 80])\n",
      "source:\n",
      "the the some germany juventus the the john canadian double federal president the un mark foreign barry new a air several president president the three congress presstek a the gary manfred un real one with the britain many the more the the the former thai zimbabwean on foreign police the brad the matt a anxiety turkey goodyear indonesia pope eu two government stock the \n",
      "\n",
      "corresponding tensor:\n",
      "tensor([    4,     4,   171,   287,  3456,     4,     4,   374,   634,  1399,\n",
      "          209,    34,     4,   184,   710,    94,  4968,    27,    10,   243,\n",
      "          543,    34,    34,     4,    81,   430, 34450,    10,     4,  4816,\n",
      "        23458,   184,   850,    74,    17,     4,   274,   511,     4,    62,\n",
      "            4,     4,     4,    90,   442,  2302,    11,    94,    65,     4,\n",
      "         5714,     4,  6649,    10,  5709,   421, 10580,   428,   968,   173,\n",
      "           47,    49,   129,     4], device='cuda:3') \n",
      "\n",
      "target:\n",
      "fda eu macau porsche juventus austria pakistan white canadian schumacher toyota obama marion un marines egyptian pressured align russian london firefighters bush under dollar students congress presstek sunni red global pranger hezbollah madrid two gulf tibet british spanish us decision healthy plum world kenny thai zimbabweans states moldovan police genetic goalkeeper india cain hutus anxious turkey goodyear # pope eu two telecom financial ioc \n",
      "\n",
      "corresponding tensor:\n",
      "tensor([ 7506,   173,  5119,  6192,  3456,  1547,   193,   389,   634,  4409,\n",
      "         2372,   272,  7250,   184,  2434,   805,  8191, 21915,   158,   386,\n",
      "         2854,   137,   233,   155,   773,   430, 34450,  2677,   611,   308,\n",
      "        34418,  2153,  1074,    47,   694,  1745,   135,   483,    48,   527,\n",
      "         4064, 21195,    50,  7000,   442,  7310,   103,  8560,    65,  4319,\n",
      "         3832,   187, 30007, 15080,  6407,   421, 10580,     3,   968,   173,\n",
      "           47,  2086,   306,  2684], device='cuda:3') \n",
      "\n",
      "tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]], device='cuda:3', dtype=torch.uint8)\n",
      "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]], device='cuda:3', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "batch = next(train_iter)\n",
    "print(type(batch), len(batch))\n",
    "print(batch[0].size(), batch[1].size(), batch[2].size(), batch[3].size())\n",
    "\n",
    "sample_source = batch[0].transpose(1,0)[0]\n",
    "sample_target = batch[1].transpose(1,0)[0]\n",
    "sample_src_mask = batch[2].transpose(1,0)[0]\n",
    "sample_tgt_mask = batch[3].transpose(1,0)[0]\n",
    "\n",
    "\n",
    "print(\"source:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(itos(sample_source, vocabulary), sample_source))\n",
    "print(\"target:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(itos(sample_target, vocabulary), sample_target))\n",
    "print(sample_src_mask)\n",
    "print(sample_tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-20 18:27:38,434 INFO pre_trained_vector_mean = 0.0019917963, pre_trained_vector_std = 0.43600857\n",
      "2019-01-20 18:27:38,439 INFO Normalizing embeddings...\n",
      "2019-01-20 18:27:38,637 INFO pre_trained_vector_mean = -1.1977131e-08, pre_trained_vector_std = 0.9999996\n"
     ]
    }
   ],
   "source": [
    "pre_trained_vector, embz_size, padding_idx = embedding_param(SAMPLE_DATA_PATH, TEXT, pre_trained_vector_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embz_size,\n",
    "    vocab_size,\n",
    "    padding_idx,\n",
    "    pre_trained_vector,\n",
    "    stack_number,\n",
    "    heads_number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-20 18:38:57,378 INFO Start training\n",
      "2019-01-20 18:40:40,687 INFO Iteration: 100, loss: 11.423600196838379, estimated remaining time: 84 min 22 s\n",
      "2019-01-20 18:42:23,178 INFO Iteration: 200, loss: 11.402462005615234, estimated remaining time: 81 min 59 s\n",
      "2019-01-20 18:44:05,617 INFO Iteration: 300, loss: 11.349199295043945, estimated remaining time: 80 min 14 s\n",
      "2019-01-20 18:45:47,634 INFO Iteration: 400, loss: 11.374692916870117, estimated remaining time: 78 min 12 s\n",
      "2019-01-20 18:47:30,265 INFO Iteration: 500, loss: 11.389216423034668, estimated remaining time: 76 min 58 s\n",
      "2019-01-20 18:49:12,842 INFO Iteration: 600, loss: 11.35831356048584, estimated remaining time: 75 min 13 s\n",
      "2019-01-20 18:50:55,074 INFO Iteration: 700, loss: 11.372986793518066, estimated remaining time: 73 min 15 s\n",
      "2019-01-20 18:52:37,488 INFO Iteration: 800, loss: 11.350546836853027, estimated remaining time: 71 min 41 s\n",
      "2019-01-20 18:54:19,887 INFO Iteration: 900, loss: 11.36902904510498, estimated remaining time: 69 min 58 s\n",
      "2019-01-20 18:56:02,298 INFO Iteration: 1000, loss: 11.385719299316406, estimated remaining time: 68 min 16 s\n",
      "2019-01-20 18:57:44,713 INFO Iteration: 1100, loss: 11.37955379486084, estimated remaining time: 66 min 34 s\n",
      "2019-01-20 18:59:27,295 INFO Iteration: 1200, loss: 11.410618782043457, estimated remaining time: 64 min 58 s\n",
      "2019-01-20 19:01:09,698 INFO Iteration: 1300, loss: 11.38796615600586, estimated remaining time: 63 min 8 s\n",
      "2019-01-20 19:02:52,118 INFO Iteration: 1400, loss: 11.37730598449707, estimated remaining time: 61 min 27 s\n",
      "2019-01-20 19:04:34,523 INFO Iteration: 1500, loss: 11.393144607543945, estimated remaining time: 59 min 44 s\n",
      "2019-01-20 19:06:16,925 INFO Iteration: 1600, loss: 11.347222328186035, estimated remaining time: 58 min 1 s\n",
      "2019-01-20 19:07:59,113 INFO Iteration: 1700, loss: 11.388106346130371, estimated remaining time: 56 min 12 s\n",
      "2019-01-20 19:09:41,537 INFO Iteration: 1800, loss: 11.422006607055664, estimated remaining time: 54 min 37 s\n",
      "2019-01-20 19:11:23,934 INFO Iteration: 1900, loss: 11.40693187713623, estimated remaining time: 52 min 54 s\n",
      "2019-01-20 19:13:06,328 INFO Iteration: 2000, loss: 11.359203338623047, estimated remaining time: 51 min 11 s\n",
      "2019-01-20 19:14:48,737 INFO Iteration: 2100, loss: 11.38077449798584, estimated remaining time: 49 min 29 s\n",
      "2019-01-20 19:16:31,148 INFO Iteration: 2200, loss: 11.343193054199219, estimated remaining time: 47 min 47 s\n",
      "2019-01-20 19:18:13,753 INFO Iteration: 2300, loss: 11.373270988464355, estimated remaining time: 46 min 10 s\n",
      "2019-01-20 19:19:56,372 INFO Iteration: 2400, loss: 11.34172248840332, estimated remaining time: 44 min 28 s\n",
      "2019-01-20 19:21:38,753 INFO Iteration: 2500, loss: 11.414787292480469, estimated remaining time: 42 min 39 s\n",
      "2019-01-20 19:23:21,174 INFO Iteration: 2600, loss: 11.379227638244629, estimated remaining time: 40 min 58 s\n",
      "2019-01-20 19:25:03,583 INFO Iteration: 2700, loss: 11.373029708862305, estimated remaining time: 39 min 15 s\n",
      "2019-01-20 19:26:46,210 INFO Iteration: 2800, loss: 11.340659141540527, estimated remaining time: 37 min 37 s\n",
      "2019-01-20 19:28:28,751 INFO Iteration: 2900, loss: 11.343939781188965, estimated remaining time: 35 min 53 s\n",
      "2019-01-20 19:30:11,201 INFO Iteration: 3000, loss: 11.380165100097656, estimated remaining time: 34 min 8 s\n",
      "2019-01-20 19:31:53,615 INFO Iteration: 3100, loss: 11.381246566772461, estimated remaining time: 32 min 25 s\n",
      "2019-01-20 19:33:35,557 INFO Iteration: 3200, loss: 11.363485336303711, estimated remaining time: 30 min 34 s\n",
      "2019-01-20 19:35:16,702 INFO Iteration: 3300, loss: 11.35728645324707, estimated remaining time: 28 min 39 s\n",
      "2019-01-20 19:36:58,921 INFO Iteration: 3400, loss: 11.380043029785156, estimated remaining time: 27 min 15 s\n",
      "2019-01-20 19:38:40,966 INFO Iteration: 3500, loss: 11.412496566772461, estimated remaining time: 25 min 30 s\n",
      "2019-01-20 19:40:23,365 INFO Iteration: 3600, loss: 11.380266189575195, estimated remaining time: 23 min 53 s\n",
      "2019-01-20 19:42:05,563 INFO Iteration: 3700, loss: 11.343342781066895, estimated remaining time: 22 min 8 s\n",
      "2019-01-20 19:43:47,567 INFO Iteration: 3800, loss: 11.37686824798584, estimated remaining time: 20 min 24 s\n",
      "2019-01-20 19:45:29,681 INFO Iteration: 3900, loss: 11.345907211303711, estimated remaining time: 18 min 43 s\n",
      "2019-01-20 19:47:11,739 INFO Iteration: 4000, loss: 11.399460792541504, estimated remaining time: 17 min 0 s\n",
      "2019-01-20 19:48:53,740 INFO Iteration: 4100, loss: 11.374478340148926, estimated remaining time: 15 min 18 s\n",
      "2019-01-20 19:50:35,393 INFO Iteration: 4200, loss: 11.395613670349121, estimated remaining time: 13 min 33 s\n",
      "2019-01-20 19:52:17,109 INFO Iteration: 4300, loss: 11.37128734588623, estimated remaining time: 11 min 52 s\n",
      "2019-01-20 19:53:58,922 INFO Iteration: 4400, loss: 11.326797485351562, estimated remaining time: 10 min 10 s\n",
      "2019-01-20 19:55:41,144 INFO Iteration: 4500, loss: 11.338640213012695, estimated remaining time: 8 min 31 s\n",
      "2019-01-20 19:57:23,094 INFO Iteration: 4600, loss: 11.359587669372559, estimated remaining time: 6 min 47 s\n",
      "2019-01-20 19:59:04,924 INFO Iteration: 4700, loss: 11.386543273925781, estimated remaining time: 5 min 5 s\n",
      "2019-01-20 20:00:46,508 INFO Iteration: 4800, loss: 11.369657516479492, estimated remaining time: 3 min 23 s\n",
      "2019-01-20 20:02:28,282 INFO Iteration: 4900, loss: 11.386560440063477, estimated remaining time: 1 min 41 s\n",
      "2019-01-20 20:04:09,142 INFO Training over.\n"
     ]
    }
   ],
   "source": [
    "optimizer = get_default_optimizer(model)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "train(model.to(device), train_iter, 5000, optimizer, criterion, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trans-1.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sum]",
   "language": "python",
   "name": "conda-env-sum-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
